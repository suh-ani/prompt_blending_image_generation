{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd6095a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suhani Rajangaonkar\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "import random\n",
    "import spacy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import ast\n",
    "import torch\n",
    "import os \n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from groq import Groq  # Assuming Groq has an OpenAI-compatible client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310ac394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load CLIP Model ===\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839708a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the client with your API key\n",
    "client=Groq(api_key=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2223a238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 11.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# ----------- Set device for Stable Diffusion -----------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ----------- Load Stable Diffusion XL base model -----------\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",  # Use if you're on a GPU like NVIDIA RTX\n",
    "    use_safetensors=True\n",
    ").to(\"cuda\")  # or your `device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e436e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIVE_PROMPT = (\n",
    "    \"blurry, deformed, ugly, bad anatomy, bad proportions, low quality, \"\n",
    "    \"mutated, out of frame, extra limbs, poorly drawn, jpeg artifacts, \"\n",
    "    \"text, watermark, signature\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "746037c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_a = \"dragon \"\n",
    "prompt_b = \"forest\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1b3d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_prompts_interactively(prompt_a, prompt_b, client):\n",
    "    \"\"\"\n",
    "    Combines two prompts into five blended prompts using LLaMA 3 model,\n",
    "    incorporating alpha weights between 0.4 and 0.6 to influence emphasis.\n",
    "\n",
    "    Parameters:\n",
    "        prompt_a (str): First input prompt \n",
    "        prompt_b (str): Second input prompt\n",
    "        client: Pre-authenticated OpenAI-style API client for llama3-70b-8192\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of prompts with keys like 'prompt_0.2', 'prompt_0.4', etc.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a visual scene composer. Your job is to merge two image generation prompts \"\n",
    "        \"into a single, meaningful visual scene. Ensure that the subjects of both prompts interact \"\n",
    "        \"in a realistic and visually coherent way. \"\n",
    "        \"if the subject is a living -there should be no facial features like eyes ,nose or mouth\"\n",
    "        \"if subject is a person then he should be fully clothed in a modern attire, his face should be realisitc with no distortions and All limbs, hands, and body proportions appear natural and anatomically correct. \"\n",
    "        \"Each time, emphasize one prompt more than the other using an alpha value between 0.4 and 0.6. Output a single descriptive sentence suitable for a text-to-image model. \"\n",
    "        \"keep less than 15 words\"\n",
    "        \"the image should be photorealistic\"\n",
    "        \"avoid AI type images \"\n",
    "        \"Return only the prompt. No explanations.\"\n",
    "    )\n",
    "\n",
    "    alphas = np.linspace(0.40, 0.60, 5)\n",
    "    prompt_dict = {}\n",
    "\n",
    "\n",
    "    for alpha in alphas:\n",
    "        alpha_rounded = round(alpha, 1)\n",
    "        user_prompt = (\n",
    "            f\"Prompt 1 (weight {1 - alpha_rounded:.1f}): {prompt_a}\\n\"\n",
    "            f\"Prompt 2 (weight {alpha_rounded:.1f}): {prompt_b}\\n\\n\"\n",
    "            \"Create one coherent prompt that blends them into an interactive visual scene, reflecting the given weights.\"\n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        combined_prompt = response.choices[0].message.content.strip()\n",
    "        var_name = f\"prompt_{alpha_rounded}\"\n",
    "        prompt_dict[var_name] = combined_prompt\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    return prompt_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe94a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "def clean_and_parse_weights(raw_response):\n",
    "    # Attempt to extract the dictionary block\n",
    "    match = re.search(r'\\{[\\s\\S]+\\}', raw_response)\n",
    "    if not match:\n",
    "        raise ValueError(\"No dictionary found in response.\")\n",
    "\n",
    "    text = match.group(0)\n",
    "\n",
    "    # Fix common malformed patterns\n",
    "    text = text.replace('\":', '\": ')           # Ensure spacing after colons\n",
    "    text = text.replace('\",', '\", ')           # Ensure spacing after commas\n",
    "    text = re.sub(r'(\\d)\\s*\"}', r'\\1\"}', text) # Fix trailing number then quote\n",
    "    text = re.sub(r'\":\\s*([0-9.]+)\"', r'\": \\1', text)  # Fix numeric values wrapped in quotes\n",
    "    text = re.sub(r'([0-9.])\"([,\\}])', r'\\1\\2', text)  # Remove trailing quote after numbers\n",
    "\n",
    "    try:\n",
    "        parsed = ast.literal_eval(text)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Cleaned version still invalid: {e}\\nCleaned:\\n{text}\")\n",
    "\n",
    "    if not isinstance(parsed, dict) or \"Final Prompt\" not in parsed:\n",
    "        raise ValueError(f\"Structure incorrect. Parsed:\\n{parsed}\")\n",
    "\n",
    "    return parsed\n",
    "\n",
    "\n",
    "def get_token_weights(final_prompt, client):\n",
    "    user_prompt = f\"\"\"\n",
    "Extract the main subjects and objects from the prompt and \n",
    "assign:\n",
    "- 1.7 to subject and object \n",
    "- 1.2 to secondary subjects and objects \n",
    "- 1.0 to other important details\n",
    "\n",
    "\"Final Prompt\": {final_prompt}\n",
    "\n",
    "Respond ONLY with a valid Python dictionary in this format:\n",
    "{{\n",
    "  \"Final Prompt\": {{\"word1\": weight, \"word2\": weight, ...}}\n",
    "}}\n",
    "\n",
    "No explanations, no extra text.\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "        temperature=0.5,\n",
    "    )\n",
    "\n",
    "    raw_response = response.choices[0].message.content.strip()\n",
    "    try:\n",
    "        return clean_and_parse_weights(raw_response)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to parse token weights: {e}\\nRaw response:\\n{raw_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "278a4fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Token Weights per Blended Prompt ---\n",
      "\n",
      "prompt_0.4:\n",
      "{'Final Prompt': {'dragon': 1.7, 'scales': 1.7, 'sunlight': 1.2, 'trees': 1.2, 'ancient': 1.0, 'majestic': 1.0, 'calmly': 1.0, 'amidst': 1.0, 'lies': 1.0, 'glistening': 1.0, 'dappled': 1.0}}\n",
      "\n",
      "prompt_0.5:\n",
      "{'Final Prompt': {'dragon': 1.7, 'forest': 1.7, 'clearing': 1.2, 'scales': 1.2, 'foliage': 1.2, 'misty': 1.0}}\n",
      "\n",
      "prompt_0.6:\n",
      "{'Final Prompt': {'dragon': 1.7, 'forest': 1.7, 'trees': 1.2, 'floor': 1.2, 'misty': 1.0}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate combined prompts\n",
    "blended_prompts = combine_prompts_interactively(prompt_a, prompt_b, client)\n",
    "\n",
    "# Step 2: Extract token weights for each combined prompt\n",
    "weights_per_prompt = {}\n",
    "\n",
    "print(\"\\n--- Token Weights per Blended Prompt ---\\n\")\n",
    "\n",
    "for name, final_prompt in blended_prompts.items():\n",
    "    try:\n",
    "        weights = get_token_weights(final_prompt, client)\n",
    "        weights_per_prompt[name] = weights\n",
    "        print(f\"{name}:\\n{weights}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract weights for {name}: {e}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49c799a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Weighted Prompts ---\n",
      "\n",
      "prompt_0.4:\n",
      "A (majestic:1.0) (dragon:1.7), with (scales:1.7) (glistening:1.0) in (dappled:1.0) (sunlight:1.2), (lies:1.0) (calmly:1.0) (amidst:1.0) (ancient:1.0) (trees:1.2).\n",
      "\n",
      "prompt_0.5:\n",
      "A majestic (dragon:1.7), (scales:1.2) glistening, lies calmly amidst dense (foliage:1.2) in a (misty:1.0) (forest:1.7) (clearing:1.2).\n",
      "\n",
      "prompt_0.6:\n",
      "A majestic (dragon:1.7) sprawls across a (misty:1.0) (forest:1.7) (floor:1.2), surrounded by towering (trees:1.2).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def apply_weights(prompt, weights_dict):\n",
    "    for word, weight in weights_dict.items():\n",
    "        # Use regex to replace whole word only, case-sensitive\n",
    "        pattern = r'\\b' + re.escape(word) + r'\\b'\n",
    "        prompt = re.sub(pattern, f\"({word}:{weight})\", prompt)\n",
    "    return prompt\n",
    "\n",
    "# Step 3: Apply weights to each blended prompt\n",
    "weighted_prompts = {}\n",
    "\n",
    "print(\"\\n--- Weighted Prompts ---\\n\")\n",
    "\n",
    "for name, final_prompt in blended_prompts.items():\n",
    "    try:\n",
    "        token_weights = weights_per_prompt[name]\n",
    "        weighted_prompt = apply_weights(final_prompt, token_weights[\"Final Prompt\"])\n",
    "        weighted_prompts[name] = weighted_prompt\n",
    "        print(f\"{name}:\\n{weighted_prompt}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to apply weights for {name}: {e}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cf03eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_0.4': 'A (majestic:1.0) (dragon:1.7), with (scales:1.7) (glistening:1.0) in (dappled:1.0) (sunlight:1.2), (lies:1.0) (calmly:1.0) (amidst:1.0) (ancient:1.0) (trees:1.2).',\n",
       " 'prompt_0.5': 'A majestic (dragon:1.7), (scales:1.2) glistening, lies calmly amidst dense (foliage:1.2) in a (misty:1.0) (forest:1.7) (clearing:1.2).',\n",
       " 'prompt_0.6': 'A majestic (dragon:1.7) sprawls across a (misty:1.0) (forest:1.7) (floor:1.2), surrounded by towering (trees:1.2).'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "749fdad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Encode Prompts Once ===\n",
    "def encode_texts(texts):\n",
    "    tokens = clip.tokenize(texts,truncate=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = clip_model.encode_text(tokens).float()\n",
    "        embeddings /= embeddings.norm(dim=-1, keepdim=True)\n",
    "    return embeddings\n",
    "\n",
    "base_embeddings = encode_texts([prompt_a, prompt_b])\n",
    "text_A, text_B = base_embeddings\n",
    "\n",
    "text_embeddings = {}  # Store embeddings per prompt\n",
    "\n",
    "for key, prompt in weighted_prompts.items():\n",
    "    embedding = encode_texts([prompt])[0]  # Single embedding\n",
    "    text_embeddings[key] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e583fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_variants = 3\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 50\n",
    "lambda_=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "834338fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_score_all(weighted_prompts, num_variants=3, num_inference_steps=50, guidance_scale=7.5, lambda_=0.5):\n",
    "    all_results = {}\n",
    "    base_embeddings = encode_texts([prompt_a, prompt_b])\n",
    "    text_A, text_B = base_embeddings  # shape: [512]\n",
    "\n",
    "    for key, prompt in weighted_prompts.items():\n",
    "        # Encode current blended prompt\n",
    "        text_blend = encode_texts([prompt])[0]\n",
    "\n",
    "        best_score = -float(\"inf\")\n",
    "        best_image = None\n",
    "        best_seed = None\n",
    "\n",
    "        for _ in range(num_variants):\n",
    "            seed = random.randint(0, 100000)\n",
    "            generator = torch.Generator(device=device).manual_seed(seed)\n",
    "            image = pipe(\n",
    "                prompt,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                generator=generator\n",
    "            ).images[0]\n",
    "\n",
    "            image_tensor = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                image_embed = clip_model.encode_image(image_tensor).float()\n",
    "                image_embed /= image_embed.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                sim_A = (image_embed @ text_A.T).item()\n",
    "                sim_B = (image_embed @ text_B.T).item()\n",
    "                sim_blend = (image_embed @ text_blend.T).item()\n",
    "\n",
    "                final_score = lambda_ * sim_blend + (1 - lambda_) * min(sim_A, sim_B)\n",
    "\n",
    "            if final_score > best_score:\n",
    "                best_score = final_score\n",
    "                best_image = image\n",
    "                best_seed = seed\n",
    "\n",
    "            print(f\"[{key}] Seed: {seed} | Score: {final_score:.4f} | sim_A: {sim_A:.4f} | sim_B: {sim_B:.4f} | sim_blend: {sim_blend:.4f}\")\n",
    "\n",
    "        all_results[key] = {\n",
    "            \"image\": best_image,\n",
    "            \"seed\": best_seed,\n",
    "            \"score\": best_score\n",
    "        }\n",
    "\n",
    "    return all_results  # Dictionary: key -> best result for each prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a5b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* Running on public URL: https://a0afe371c00be83b88.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a0afe371c00be83b88.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (84 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['( trees : 1 . 2 ).']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted / Blended Prompts:\n",
      "α = prompt_0.4: A (corgi:1.7) (puppy:1.7) (plays:1.0) (near:1.0) the (serene:1.0) (lake:1.2)'s (edge:1.0), (surrounded:1.0) by (sunflowers:1.2) and (autumn:1.2) (trees:1.2).\n",
      "α = prompt_0.5: A (corgi:1.7) (puppy:1.7) (plays:1.0) near a (lake:1.2)'s (edge:1.2), (surrounded:1.0) by (sunflowers:1.2) and (reflected:1.0) (autumn:1.2) (trees:1.0).\n",
      "α = prompt_0.6: A (corgi:1.7) (puppy:1.7) sits at the (lake:1.2)'s edge, surrounded by vibrant (autumn:1.2) (trees:1.2), reflected in calm (waters:1.2).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:10<00:00,  4.57it/s]\n",
      "C:\\Users\\Suhani Rajangaonkar\\AppData\\Local\\Temp\\ipykernel_22564\\1016584335.py:29: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3687.)\n",
      "  sim_A = (image_embed @ text_A.T).item()\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['( trees : 1 . 2 ).']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.4] Seed: 53129 | Score: 0.2353 | sim_A: 0.1769 | sim_B: 0.1831 | sim_blend: 0.2936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.28it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['( trees : 1 . 2 ).']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.4] Seed: 58521 | Score: 0.2134 | sim_A: 0.1657 | sim_B: 0.1704 | sim_blend: 0.2610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.36it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [').']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.4] Seed: 47814 | Score: 0.2625 | sim_A: 0.1914 | sim_B: 0.1968 | sim_blend: 0.3336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.33it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [').']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.5] Seed: 79871 | Score: 0.2711 | sim_A: 0.1898 | sim_B: 0.1913 | sim_blend: 0.3523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.34it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [').']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.5] Seed: 87265 | Score: 0.2543 | sim_A: 0.1791 | sim_B: 0.1935 | sim_blend: 0.3295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.5] Seed: 86308 | Score: 0.2720 | sim_A: 0.1862 | sim_B: 0.2010 | sim_blend: 0.3578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.6] Seed: 15812 | Score: 0.2299 | sim_A: 0.1824 | sim_B: 0.2351 | sim_blend: 0.2775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.6] Seed: 34240 | Score: 0.2841 | sim_A: 0.1938 | sim_B: 0.2155 | sim_blend: 0.3743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.6] Seed: 51351 | Score: 0.2862 | sim_A: 0.1989 | sim_B: 0.2299 | sim_blend: 0.3734\n",
      "Weighted / Blended Prompts:\n",
      "α = prompt_0.4: A 1960s vintage (car:1.7) overlooks a (serene:0) (autumn:1.2) (lake:1.7) through (Parisian:1.0) (streets:1.2)' (misty:1.0) (veil:1.2).\n",
      "α = prompt_0.5: A 1960s vintage (car:1.7) overlooks a serene (autumn:1.0)-colored (lake:1.7) surrounded by (trees:1.2) in the French (countryside:1.2).\n",
      "α = prompt_0.6: A 1960s (vintage:1.0) (car:1.7) overlooks a (serene:1.0) (autumn:1.2) (lake:1.7), (surrounded:1.0) (by:1.0) vibrant (trees:1.7), in (Paris:1.2).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:10<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.4] Seed: 10322 | Score: 0.2665 | sim_A: 0.1824 | sim_B: 0.2206 | sim_blend: 0.3506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:10<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.4] Seed: 43003 | Score: 0.2583 | sim_A: 0.1797 | sim_B: 0.2008 | sim_blend: 0.3369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:10<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.4] Seed: 76022 | Score: 0.2397 | sim_A: 0.1657 | sim_B: 0.1823 | sim_blend: 0.3138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.5] Seed: 95670 | Score: 0.2478 | sim_A: 0.1853 | sim_B: 0.2250 | sim_blend: 0.3103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.5] Seed: 96815 | Score: 0.2455 | sim_A: 0.1812 | sim_B: 0.2232 | sim_blend: 0.3098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.5] Seed: 88363 | Score: 0.2377 | sim_A: 0.1815 | sim_B: 0.2243 | sim_blend: 0.2940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.6] Seed: 54239 | Score: 0.2550 | sim_A: 0.1703 | sim_B: 0.1938 | sim_blend: 0.3397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.6] Seed: 87661 | Score: 0.2691 | sim_A: 0.1722 | sim_B: 0.2016 | sim_blend: 0.3659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0.6] Seed: 31573 | Score: 0.2756 | sim_A: 0.1883 | sim_B: 0.2293 | sim_blend: 0.3628\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import gradio as gr\n",
    "def generate_final_image(prompt1, prompt2):\n",
    "    # Step 1: Blend prompts\n",
    "    blended_prompts = combine_prompts_interactively(prompt1, prompt2, client)\n",
    "\n",
    "    # Step 2: Get token weights for each blended prompt\n",
    "    weights_dicts = {\n",
    "        k: get_token_weights(v, client)[\"Final Prompt\"]\n",
    "        for k, v in blended_prompts.items()\n",
    "    }\n",
    "\n",
    "    # Step 3: Apply weights to each prompt\n",
    "    weighted_prompts = {\n",
    "        k: apply_weights(blended_prompts[k], weights_dicts[k])\n",
    "        for k in blended_prompts\n",
    "    }\n",
    "\n",
    "    print(\"Weighted / Blended Prompts:\")\n",
    "    for k, v in weighted_prompts.items():\n",
    "        print(f\"α = {k}: {v}\")\n",
    "\n",
    "    # Step 4: Generate & score images\n",
    "    results = generate_and_score_all(\n",
    "        weighted_prompts,\n",
    "        num_variants=3,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.5,\n",
    "        lambda_=0.5\n",
    "    )\n",
    "\n",
    "    # Step 5: Select best image\n",
    "    best_key = max(results, key=lambda k: results[k][\"score\"])\n",
    "    best_image = results[best_key][\"image\"]\n",
    "\n",
    "    return best_image\n",
    "\n",
    "\n",
    "# Launch Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=generate_final_image,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Prompt 1\"),\n",
    "        gr.Textbox(label=\"Prompt 2\")\n",
    "    ],\n",
    "    outputs=gr.Image(type=\"pil\"),\n",
    "    title=\"Prompt Blending Image Generator\",\n",
    "    description=\"Enter two prompts. The system will blend them, generate multiple images, and return the best one.\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c245f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
