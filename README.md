# Prompt_blending_image_generation

This project explores a novel approach to text-to-image generation by leveraging Large Language Models (LLMs) and Stable Diffusion to automatically determine and apply semantic blending between two textual prompts. Unlike traditional prompt-to-image systems that rely on user-defined weights for controlling the influence of different concepts, this project automates the weighting process by utilizing an LLM to interpret and emphasize the most relevant subjects within a prompt. The core methodology involves  two distinct prompts and blending them using a series of weighted interpolations controlled by an alpha parameter (ranging from 0 to 1). For each alpha value, a blended prompt embedding is computed using CLIP (Contrastive Language–Image Pretraining) embeddings. These embeddings are then passed through a Stable Diffusion pipeline to generate corresponding images. To identify the most semantically aligned result, the generated images are compared using cosine similarity with the original prompts’ embeddings. The image with the highest similarity score is selected as the best representation of the blended concept.
